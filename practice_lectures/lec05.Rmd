---
title: "Lecture 4 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 14, 2018"
---


## Computer Science Sidebar

Before we get back to modeling, it is worth knowing a couple more data structures in R. These are not "data [science] types", these are "[computer science] data types". 

The first are "lists" which are "ordered hashmaps" or "ordered dictionaries" or "hash tables". if You don't know what this is, you should read about this online as it should have been covered in a intro to CS class.

```{r}
dict = list()
dict$a = "first"
dict$b = "second"
dict$c = "third"
dict
length(dict)
names(dict) #keys
dict_unlisted = unlist(dict) #values
dict_unlisted
class(dict_unlisted) #i.e. a vector
#three ways to access values by key / ordered location
dict$a
dict[["a"]]
dict[[1]] #value of first entered key
class(dict[[1]])
dict[[1 : 2]] #bombs
#now let's try to access a value for a non-existent key / ordered location
dict$q
dict[["q"]]
dict[[4]] #bombs
#convenient means to subset the list
dict[1]
class(dict[1])
dict[1 : 2]
dict[1 : 4] #this is the reason this type of access is not recommended
dict = list("first", "second", "third") #no key => value... what happens?
dict #default keys are the numbers 1, 2, ...
dict[[1]]
```

Lists conveniently allow all sorts of data types.

```{r}
varied_dict = list()
varied_dict$a = "first"
varied_dict$b = 2
varied_dict$c = 1 : 7
varied_dict$d = matrix(NA, nrow = 2, ncol = 2)
varied_dict[["the last entry"]] = function(x){x^2} #this key is not recommended
varied_dict
varied_dict$`the last entry` #note the tick marks (sometimes seen) needed due to spaces in key name
length(varied_dict)
names(varied_dict)
```

They have lots of uses in data science applications. We will likely see them in class and if not, you'll definitely see them in the real world. Note that data.frame objects are implemented as lists as well as many other common R objects.


The second is arrays i.e. are multidimensional vectors

```{r}
x = array(1 : 5, 5)
x
X = array(1 : 25, dim = c(5, 5))
X
X = array(1 : 125, dim = c(5, 5, 5))
X
X[1, , ]
X[, 1, ]
X[, , 1]
X = array(1 : 125, dim = c(5, 5, 5))
X
```

These can be associative arrays too and operate like a hash of vectors across arbitrary dimensions:

```{r}
X = array(1 : 125, 
          dim = c(5, 5, 5),
          dimnames = list(
            c("A", "B", "C", "D", "E"),
            c("I", "II", "III", "IV", "V"),
            c("blue", "red", "green", "yellow", "orange")
          ))
X
X["A", , ]
X[, "III", ]
X[, , "orange"]
X["C", , "orange"]
X["C", "IV", "orange"]
```



## Saving and Loading in R

Let's save one object:

```{r}
save(varied_dict, file = "varied_dict.RData") #RData is recommended extension for convenience
rm(varied_dict)
varied_dict
load("varied_dict.RData")
varied_dict
```

Or save the whole workspace:

```{r}
save.image("all_work.RData")
rm(list = ls())
ls() #empty vector i.e. nothing here!
load("all_work.RData")
ls()
```

```{r}
rm(list = ls()) #cleanup everything
```

## Matrix operations in R

R can do all the standard matrix operations. Let's go through them quickly. First initialize two example matrices:

```{r}
A = matrix(rep(1, 4), nrow = 2)
A
B = array(seq(1, 4), dim = c(2, 2))
B
I = diag(2) #create an identity matrix of size 2x2
I
```

Now we show off some operations:

```{r}
A * B #element-wise multiplication
A %*% B #matrix multiplication
B %*% I
t(B) #transpose
solve(B)
solve(A) #BOOM - why?
solve(I)
#rank(A) = 1 #no such function... but... there are tons of add-on libraries for matrix computations e.g.
pacman::p_load(Matrix) #load the Matrix library
rankMatrix(B)
rankMatrix(A)
rankMatrix(I)
#technical note, skip
?attr #not recommended... return as lists!
```

Note that vectors and matrices are not the same:

```{r}
v = c(1, 2, 3) #3-d vector
t(v) #converts to 1x3 vector... unsure why
t(t(v))
v %*% v #seems to default to dot product
t(v) %*% t(t(v)) #dot product
I = diag(3)
I %*% v #seems to default correctly!
I %*% t(v) #actually uncomformable
```

We can also do eigen decomposition very easily:

```{r}
eigen_decomp = eigen(B)
V = eigen_decomp$vectors
v1 = V[, 1, drop = FALSE]
v2 = V[, 2, drop = FALSE]
lambdas = eigen_decomp$values
lambda1 = lambdas[1]
lambda2 = lambdas[2]

B %*% v1
lambda1 * v1

B %*% v2 
lambda2 * v2
B %*% v2 == lambda2 * v2 #why not?

B
V %*% diag(lambdas) %*% solve(V)
```

We'll return to eigenvalues/vectors at some point in the future. Hopefully when we do principal components analysis.


## The Perceptron

Time for some new data first... we are bored of the fabricated creditworthiness data.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 9] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

Now let's run the "perceptron learning algorithm" for one dimension - just V1. You will do an example with more features for homework.

```{r}
MAX_ITER = 1000
w_vec = rep(0, 2) #intialize a 2-dim vector

X1 = as.matrix(cbind(1, X[, 1, drop = FALSE]))

for (iter in 1 : MAX_ITER){  
  for (i in 1 : nrow(X1)){
    x_i = X1[i, ]
    yhat_i = ifelse(sum(x_i * w_vec) > 0, 1, 0)
    y_i = y_binary[i]
    w_vec = w_vec + (y_i - yhat_i) * x_i
  }
}
w_vec
```

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```





## The Perceptron (again)

We spoke about how the w vector is in a large space. A course in optimization will describe methods how to find optimal and approximately optimal solutions for `w` based on an "objective function" or "fitness function" or "cost function". Here that target is the number of total errors of the `n` examples in the training data set.

Here are some off-the-shelf solvers in R in action. Note: this is not the classic "perceptron learning algorithm". Also: this is complete junk made-up data so it's only an illustration. You will see real data for your homework.

```{r}
SAE = function(w_vec){
  yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
  sum(y_binary != yhat)
}

?nlm
w_vec = nlm(SAE, c(1, 1))$estimate #doesn't work at all ... I think because our cost function is not continuous
w_vec

pacman::p_load(neldermead)
?optim
optim_output = optim(c(0, 0), SAE)
optim_output
w_vec = optim_output$par
```

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Lesson: perceptron learning algorithm maybe not so great...

## Nearest Neighbor algorithm

In one dimension, we are looking for the closest x.

```{r}
g_function = function(x_star){
  best_sqd_distance = Inf #good place to begin
  i_star = NA
  for (i in 1 : nrow(X)){
    dsqd = (X[i, 1] - x_star)^2
    if (dsqd < best_sqd_distance){
      best_sqd_distance = dsqd
      i_star = i
    }
  }
  y_binary[i_star]
}
g_function(7.8)
g_function(5.2)
```

Why is this silly for this dataset?

```{r}
str(X)
```
The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1), y_binary, k = 1)
y_hat
```

Now for an interesting exercise that will setup future classes:

```{r}

y_hat = knn(X, X, y_binary, k = 1)
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No...

This is the first example of "overfitting". We will explore this in depth.
