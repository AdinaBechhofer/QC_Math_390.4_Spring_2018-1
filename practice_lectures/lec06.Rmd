---
title: "Lecture 6 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 20, 2018"
---

First we load up the breast cancer data set.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = Xy[, 2 : 9] #V1, V2, ..., V9
head(X)
y_binary = as.numeric(Xy$class == "malignant")
table(y_binary)
```

We should get a baseline when $g = 0$ or $g = 1$ of the error rates on $\mathbb{D}$.

```{r}
#If always 0, the all 1's are errors
239 / (444 + 239)
#If always 1, the all 0's are errors
444 / (444 + 239)
```

## Nearest Neighbor algorithm

In one dimension, we are looking for the closest x.

```{r}
g_function = function(x_star){
  best_sqd_distance = Inf #good place to begin
  i_star = NA
  for (i in 1 : nrow(X)){
    dsqd = (X[i, 1] - x_star)^2
    if (dsqd < best_sqd_distance){
      best_sqd_distance = dsqd
      i_star = i
    }
  }
  y_binary[i_star]
}
g_function(7.8)
g_function(5.2)
```

Why is this silly for this dataset?

```{r}
str(X)
```
The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1), y_binary, k = 1)
y_hat
```

Now for an interesting exercise that will setup future classes:

```{r}

y_hat = knn(X, X, y_binary, k = 1)
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No...

This is the first example of "overfitting". We will explore this in depth.
